{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation Algorithm From Scratch\n",
    "\n",
    "The idea is to implement the **Back Propagation** algorithm to train Feed Forward Neural Networks. \n",
    "\n",
    "We will implement the algorithm from scratch in Python, and we will only use built-in libraries and numpy, avoiding for the moment the usage of higher-level frameworks (Pytorch or Tensorflow).\n",
    "\n",
    "This excersice will help me remeber the foundations of Deep Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "The **Loss Function** for the moment is the $l_2$ norm of the difference between the predicted and true labels ($\\hat y$ and $y$, respecitvely).\n",
    "\n",
    "$$\n",
    "L(y, \\hat y) = \\|y-\\hat y \\|^2\n",
    "$$\n",
    "\n",
    "### Feed Forward Model\n",
    "\n",
    "The **Feed Forward** model considers the consecutive application of linear transformations and activation functions. Given an input $x$, the model's output is given by:\n",
    "\n",
    "$$\n",
    "g(x) = f_K(f_{K-1}(\\cdots f_2(f_1(x))))\n",
    "$$\n",
    "\n",
    "where:\n",
    "$$\n",
    "\\\\\n",
    "f_0 = x,  \\\\\n",
    "f_i = \\sigma(A_{i-1}f_{i-1}+b_{i-1})\n",
    "$$\n",
    "\n",
    "\n",
    "For simplicity, we consider the activation functions to be the sigmoid function:\n",
    "\n",
    "$$\n",
    "\\sigma (x) = \\dfrac{1}{1+e^{-x}},  \\\\\n",
    "\\sigma ' (x) = \\sigma (x) (1-\\sigma (x))\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Learning using Gradient Descent\n",
    "\n",
    "\n",
    "Notice that the model $g(x)$ depends on the parameters $\\{A_0, b_0, \\cdots, A_{K-1}, b_{K-1}\\}$ that represent the weights ($A_i$) and the biases ($b_i$) of the model. Such parameters are optimized using a Gradient Descent algorithm, which updates the values of the parameters iteratively by following the rule:\n",
    "\n",
    "\n",
    "$$\n",
    "A_i \\leftarrow A_i - \\alpha \\dfrac{\\partial L}{\\partial A_i} \\\\\n",
    "b_i \\leftarrow b_i - \\alpha \\dfrac{\\partial L}{\\partial b_i}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "\n",
    "Hence, the question is: How do we estimate $\\dfrac{\\partial L}{\\partial A_i}$? The answer is the Backpropagation algorithm, which in turn is based on a reverse application of the Chain Rule for diferentiation.\n",
    "\n",
    "### Chain Rule\n",
    "\n",
    "\n",
    "Consider the model for $g(x)$ by using the following auxiliary variables:\n",
    "\n",
    "$$\n",
    "h_i = A_{i-1}f_{i-1}+b_{i-1} \\\\\n",
    "f_i = \\sigma (h_i)\n",
    "$$\n",
    "\n",
    "where the boundary conditions is $f_0 = x$.\n",
    "\n",
    "The Chain Rule states:\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial A_{K-1}} = \\dfrac{\\partial L}{\\partial f_K} \\dfrac{\\partial f_K}{\\partial h_K} \\dfrac{\\partial h_K}{\\partial A_{K-1}} \\\\\n",
    "\\dfrac{\\partial L}{\\partial A_{K-2}} = \\dfrac{\\partial L}{\\partial f_K} (\\dfrac{\\partial f_K}{\\partial h_K} \\dfrac{\\partial h_K}{\\partial f_{K-1}} ) (\\dfrac{\\partial f_{K-1}}{\\partial h_{K-1}} \\dfrac{\\partial h_{K-1}}{\\partial A_{K-2}} )\\\\\n",
    "\\dfrac{\\partial L}{\\partial A_{K-3}} = \\dfrac{\\partial L}{\\partial f_K} (\\dfrac{\\partial f_K}{\\partial h_K} \\dfrac{\\partial h_K}{\\partial f_{K-1}} ) (\\dfrac{\\partial f_{K-1}}{\\partial h_{K-1}} \\dfrac{\\partial h_{K-1}}{\\partial f_{K-2}} ) (\\dfrac{\\partial f_{K-2}}{\\partial h_{K-2}} \\dfrac{\\partial h_{K-2}}{\\partial A_{K-3}} )\\\\\n",
    "\\dfrac{\\partial L}{\\partial A_{i}} = \\dfrac{\\partial L}{\\partial f_K} (\\dfrac{\\partial f_K}{\\partial h_K} \\dfrac{\\partial h_K}{\\partial f_{K-1}} ) (\\dfrac{\\partial f_{K-1}}{\\partial h_{K-1}} \\dfrac{\\partial h_{K-1}}{\\partial f_{K-2}} )\\cdots(\\dfrac{\\partial f_{i+2}}{\\partial h_{i+1}} \\dfrac{\\partial h_{i+1}}{\\partial f_{i+1} }) (\\dfrac{\\partial f_{i+1}}{\\partial h_{i+1}} \\dfrac{\\partial h_{i+1}}{\\partial A_{i} })\\\\\n",
    "$$\n",
    "\n",
    "\n",
    "Some of the terms are:\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial f_K}  = \\dfrac{\\partial }{\\partial f_K} \\| y-f_K\\|^2 = -2 (y-f_k)I \\\\\n",
    "\\dfrac{\\partial f_i}{\\partial h_i} = \\sigma (h_i) (1-\\sigma (h_i)) \\\\\n",
    "\\dfrac{\\partial h_i}{\\partial f_{i-1}} = A_{i-1} \\\\\n",
    "\\dfrac{\\partial h_i}{\\partial A_{i-1}} = f_i \\otimes I\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy example\n",
    "\n",
    "Consider $K=2$.\n",
    "\n",
    "The computational graph is:\n",
    "\n",
    "$$\n",
    "f_0 = x \\\\\n",
    "\\rightarrow h_1 = A_0x+b_0  \\\\\n",
    "\\rightarrow f_1 = \\sigma (h_1) \\\\\n",
    "\\rightarrow h_2 = A_1 f_2+b_1 \\\\\n",
    "\\rightarrow f_2 = \\sigma (h_2)\n",
    "$$\n",
    "\n",
    "Then, the gradients are:\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial A_1} = \\dfrac{\\partial L}{\\partial f_2} \\dfrac{\\partial f_2}{\\partial h_2} \\dfrac{\\partial h_2}{\\partial A_1} = -2(y-f_2)I \\cdot \\sigma (h_2) (1-\\sigma (h_2)) \\cdot f_2 \\otimes I \\\\\n",
    "\\dfrac{\\partial L}{\\partial A_0} = \\dfrac{\\partial L}{\\partial f_2} \\dfrac{\\partial f_2}{\\partial h_2}\\dfrac{\\partial h_2}{\\partial f_1} \\dfrac{\\partial f_1}{\\partial h_1} \\dfrac{\\partial h_1}{\\partial A_0} = -2(y-f_2)I \\cdot \\sigma (h_2) (1-\\sigma (h_2)) \\cdot A_1 \\cdot \\sigma (h_1) (1-\\sigma (h_1)) \\cdot f_1 \\otimes I\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To DO -  First Implementation (Toy Example):\n",
    "\n",
    "We will start with a simple regression model, using a feed-forward NN with only 2 hidden layers. \n",
    "\n",
    "The objective is to derive the backprop equations by hand, and implement them transparently. Then. we will implement a generalization of the algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Load training/test data X/Y\n",
    "    2. Create a NN using Feed Forward Architechture\n",
    "    3. Calculate A loss Function\n",
    "    4. Implement the optimization of the Loss Function through Backpropagation (Derive the math)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sklearn.datasets.load_boston(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD6CAYAAABamQdMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQMUlEQVR4nO3dfayedX3H8fdnoPg0I6ynrLZlB5fqLManHAkb24Ki0g1C+YekJC7NRtJsYQ43jRb9g2xJk+4hziWbSxrp6CKDNIrSqHN2VceWKOzw4KAURiMMaivnOOLUbcEVv/vjXA03h7ucc+6HHvid9ytp7vv6Xtd1ri+/hM/55Xeu+7pTVUiS2vJTy92AJGn0DHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYtGO5JdieZSXL/vPr7kzyU5GCSP+mpX5fkcLfvknE0LUl6fqcv4pgbgb8E/vZEIck7gc3Am6vqqSSru/pGYAtwHvBa4B+TvL6qnn6+C6xataomJycH+g+QpJXqrrvu+l5VTfTbt2C4V9XtSSbnlX8H2FlVT3XHzHT1zcAtXf2RJIeB84FvPN81JicnmZ6eXqgVSVKPJP9xsn2Drrm/HviVJHck+ack7+jqa4HHe4470tUkSafQYpZlTnbemcAFwDuAvUleB6TPsX2fb5BkG7AN4JxzzhmwDUlSP4PO3I8At9acO4GfAKu6+vqe49YBR/v9gKraVVVTVTU1MdF3yUiSNKBBw/3zwLsAkrweeCnwPWAfsCXJGUnOBTYAd46iUUnS4i24LJPkZuAiYFWSI8D1wG5gd3d75I+BrTX3eMmDSfYCDwDHgWsWulNGkjR6eSE88ndqaqq8W0aSlibJXVU11W+fn1CVpAYZ7pLUIMNdkho06H3uWqEmt39xWa776M5Ll+W60ouVM3dJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1KAFwz3J7iQz3felzt/3oSSVZFVP7bokh5M8lOSSUTcsSVrYYmbuNwKb5heTrAfeAzzWU9sIbAHO6875ZJLTRtKpJGnRFgz3qrodeLLPrj8HPgz0fsP2ZuCWqnqqqh4BDgPnj6JRSdLiDbTmnuRy4DtV9a15u9YCj/dsH+lqkqRTaMlfs5fkFcDHgPf2292nVn1qJNkGbAM455xzltqGJOl5DDJz/3ngXOBbSR4F1gF3J/lZ5mbq63uOXQcc7fdDqmpXVU1V1dTExMQAbUiSTmbJ4V5V91XV6qqarKpJ5gL97VX1XWAfsCXJGUnOBTYAd460Y0nSghZzK+TNwDeANyQ5kuTqkx1bVQeBvcADwJeBa6rq6VE1K0lanAXX3KvqqgX2T87b3gHsGK4tSdIw/ISqJDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGLeY7VHcnmUlyf0/tT5M8mOTfknwuyWt69l2X5HCSh5JcMq7GJUknt5iZ+43Apnm1/cCbqurNwL8D1wEk2QhsAc7rzvlkktNG1q0kaVEWDPequh14cl7tK1V1vNv8JrCue78ZuKWqnqqqR4DDwPkj7FeStAijWHP/LeDvu/drgcd79h3pas+RZFuS6STTs7OzI2hDknTCUOGe5GPAceCmE6U+h1W/c6tqV1VNVdXUxMTEMG1IkuY5fdATk2wFLgMurqoTAX4EWN9z2Drg6ODtSZIGMdDMPckm4CPA5VX1Pz279gFbkpyR5FxgA3Dn8G1KkpZiwZl7kpuBi4BVSY4A1zN3d8wZwP4kAN+sqt+uqoNJ9gIPMLdcc01VPT2u5iVJ/S0Y7lV1VZ/yDc9z/A5gxzBNSZKG4ydUJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1aMFwT7I7yUyS+3tqZyXZn+Th7vXMnn3XJTmc5KEkl4yrcUnSyS1m5n4jsGlebTtwoKo2AAe6bZJsBLYA53XnfDLJaSPrVpK0KAuGe1XdDjw5r7wZ2NO93wNc0VO/paqeqqpHgMPA+SPqVZK0SIOuuZ9dVccAutfVXX0t8HjPcUe62nMk2ZZkOsn07OzsgG1IkvoZ9R9U06dW/Q6sql1VNVVVUxMTEyNuQ5JWtkHD/YkkawC615mufgRY33PcOuDo4O1JkgYxaLjvA7Z277cCt/XUtyQ5I8m5wAbgzuFalCQt1ekLHZDkZuAiYFWSI8D1wE5gb5KrgceAKwGq6mCSvcADwHHgmqp6eky9S5JOYsFwr6qrTrLr4pMcvwPYMUxTkqTh+AlVSWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDFvyyDumFYHL7F5ft2o/uvHTZri0Nypm7JDVoqHBP8vtJDia5P8nNSV6W5Kwk+5M83L2eOapmJUmLM/CyTJK1wO8BG6vqf7svxt4CbAQOVNXOJNuB7cBHRtKtgOVdopD04jDssszpwMuTnA68AjgKbAb2dPv3AFcMeQ1J0hINHO5V9R3gz4DHgGPAf1XVV4Czq+pYd8wxYHW/85NsSzKdZHp2dnbQNiRJfQwc7t1a+mbgXOC1wCuTvG+x51fVrqqaqqqpiYmJQduQJPUxzLLMu4FHqmq2qv4PuBX4JeCJJGsAuteZ4duUJC3FMOH+GHBBklckCXAxcAjYB2ztjtkK3DZci5KkpRr4bpmquiPJZ4C7gePAPcAu4FXA3iRXM/cL4MpRNCpJWryhPqFaVdcD188rP8XcLF6StEz8hKokNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYNFe5JXpPkM0keTHIoyS8mOSvJ/iQPd69njqpZSdLiDDtz/wvgy1X1C8BbgEPAduBAVW0ADnTbkqRTaOBwT/Jq4FeBGwCq6sdV9X1gM7CnO2wPcMWwTUqSlmaYmfvrgFngb5Lck+RTSV4JnF1VxwC619X9Tk6yLcl0kunZ2dkh2pAkzTdMuJ8OvB3466p6G/DfLGEJpqp2VdVUVU1NTEwM0YYkab5hwv0IcKSq7ui2P8Nc2D+RZA1A9zozXIuSpKUaONyr6rvA40ne0JUuBh4A9gFbu9pW4LahOpQkLdnpQ57/fuCmJC8Fvg38JnO/MPYmuRp4DLhyyGtIkpZoqHCvqnuBqT67Lh7m50qShuMnVCWpQcMuy6xok9u/uNwtSFJfztwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0aOtyTnJbkniRf6LbPSrI/ycPd65nDtylJWopRzNyvBQ71bG8HDlTVBuBAty1JOoWG+pq9JOuAS4EdwB905c3ARd37PcDXgY8Mcx1pOS3X1yk+uvPSZbmu2jDszP0TwIeBn/TUzq6qYwDd6+p+JybZlmQ6yfTs7OyQbUiSeg0c7kkuA2aq6q5Bzq+qXVU1VVVTExMTg7YhSepjmGWZC4HLk/w68DLg1Uk+DTyRZE1VHUuyBpgZRaOSpMUbeOZeVddV1bqqmgS2AF+tqvcB+4Ct3WFbgduG7lKStCTjuM99J/CeJA8D7+m2JUmn0FB3y5xQVV9n7q4Yquo/gYtH8XMlSYPxE6qS1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDVoJPe5Sxq95XoaJfhEyhY4c5ekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYNHO5J1if5WpJDSQ4mubarn5Vkf5KHu9czR9euJGkxhpm5Hwc+WFVvBC4ArkmyEdgOHKiqDcCBbluSdAoNHO5Vdayq7u7e/xA4BKwFNgN7usP2AFcM26QkaWlG8uCwJJPA24A7gLOr6hjM/QJIsnoU13g+y/mAJUkvfi0+pG3oP6gmeRXwWeADVfWDJZy3Lcl0kunZ2dlh25Ak9Rgq3JO8hLlgv6mqbu3KTyRZ0+1fA8z0O7eqdlXVVFVNTUxMDNOGJGmeYe6WCXADcKiqPt6zax+wtXu/Fbht8PYkSYMYZs39QuA3gPuS3NvVPgrsBPYmuRp4DLhyuBYlSUs1cLhX1b8AOcnuiwf9uZKk4fkJVUlqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDRvJUSEkaBZ/wOjrO3CWpQYa7JDXIZRlJz+HyyIufM3dJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUoLGFe5JNSR5KcjjJ9nFdR5L0XGMJ9ySnAX8F/BqwEbgqycZxXEuS9FzjmrmfDxyuqm9X1Y+BW4DNY7qWJGmecYX7WuDxnu0jXU2SdAqM6/ED6VOrZx2QbAO2dZs/SvLQmHo5VVYB31vuJl5AHI9nczye4Vj0yB8PNR4/d7Id4wr3I8D6nu11wNHeA6pqF7BrTNc/5ZJMV9XUcvfxQuF4PJvj8QzH4tnGNR7jWpb5V2BDknOTvBTYAuwb07UkSfOMZeZeVceT/C7wD8BpwO6qOjiOa0mSnmtsj/ytqi8BXxrXz38BamaJaUQcj2dzPJ7hWDzbWMYjVbXwUZKkFxUfPyBJDTLcB5Bkd5KZJPf31M5Ksj/Jw93rmcvZ46mSZH2SryU5lORgkmu7+kodj5cluTPJt7rx+MOuviLHA+Y+sZ7kniRf6LZX8lg8muS+JPcmme5qYxkPw30wNwKb5tW2AweqagNwoNteCY4DH6yqNwIXANd0j5pYqePxFPCuqnoL8FZgU5ILWLnjAXAtcKhneyWPBcA7q+qtPbc/jmU8DPcBVNXtwJPzypuBPd37PcAVp7SpZVJVx6rq7u79D5n7n3gtK3c8qqp+1G2+pPtXrNDxSLIOuBT4VE95RY7F8xjLeBjuo3N2VR2DucADVi9zP6dckkngbcAdrODx6JYh7gVmgP1VtZLH4xPAh4Gf9NRW6ljA3C/6ryS5q/uUPoxpPMZ2K6RWliSvAj4LfKCqfpD0ewLFylBVTwNvTfIa4HNJ3rTcPS2HJJcBM1V1V5KLlrufF4gLq+poktXA/iQPjutCztxH54kkawC615ll7ueUSfIS5oL9pqq6tSuv2PE4oaq+D3ydub/PrMTxuBC4PMmjzD0Z9l1JPs3KHAsAqupo9zoDfI65J+iOZTwM99HZB2zt3m8FblvGXk6ZzE3RbwAOVdXHe3at1PGY6GbsJHk58G7gQVbgeFTVdVW1rqommXsEyVer6n2swLEASPLKJD994j3wXuB+xjQefohpAEluBi5i7ul2TwDXA58H9gLnAI8BV1bV/D+6NifJLwP/DNzHM+uqH2Vu3X0ljsebmfuj2GnMTZ72VtUfJfkZVuB4nNAty3yoqi5bqWOR5HXMzdZhbkn876pqx7jGw3CXpAa5LCNJDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lq0P8D44Bzd9xvh4kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, input_dim, hidden_layers, output_dim):\n",
    "        \n",
    "        # Shapes of input, output and weight matrices\n",
    "        self.i_d = input_dim\n",
    "        self.o_d = output_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        \n",
    "        # Init weights\n",
    "        self.init_weights()\n",
    "        \n",
    "    \n",
    "    # Loss Function\n",
    "    def loss(self, y_true, y_pred):\n",
    "        return np.linalg.norm(y_true-y_pred)\n",
    "    \n",
    "    # Initialize random weights\n",
    "    def init_weights(self):\n",
    "        layers =  [self.i_d]+self.hidden_layers+[self.o_d]\n",
    "        self.weights = [np.random.rand(layers[i+1], layers[i]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.random.rand(layers[i+1]) for i in range(len(layers)-1)]\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return np.exp(x)/(1+np.exp(x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return self.sigmoid(x)*(1.0-self.sigmoid(x))\n",
    "\n",
    "    def forward_propagation(self, x):\n",
    "        z = np.copy(x)\n",
    "        for (w,b) in zip(self.weights, self.biases):\n",
    "            h = np.matmul(w, z)+b\n",
    "            print(h)\n",
    "            z = self.sigmoid(h)\n",
    "        return z\n",
    "        \n",
    "    def backward_propagation(self, x, y):\n",
    "        return None\n",
    "    \n",
    "    def train(self):\n",
    "        pass\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X.shape[1]\n",
    "output_dim = 1\n",
    "layers = [16, 8, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[195.30904155 308.60588814 424.82504162 221.90710708 314.64799731\n",
      " 473.4865447  436.90112073 434.20011785 326.17703638 549.8328171\n",
      " 539.0789653  327.29832239 379.12563715 165.09029629 458.23255385\n",
      " 438.00229204]\n",
      "[ 9.22073266 10.04812776  8.32545072 11.66573707  9.74218925  8.24978706\n",
      "  7.93529688  5.62032806]\n",
      "[6.32492248 6.06050001 5.06877668 3.49547883]\n",
      "[2.35285036]\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(input_dim, layers, output_dim)\n",
    "z = model.forward_propagation(X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14854.18677312])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
